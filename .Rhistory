am.t.test
summary(am.t.test)
library(MASS)
?shuttle
names(shuttle)
?auto
model <- glm(use~sign,data=shuttle,model='binomial')
help glm
?glm
model <- glm(use~sign,data=shuttle,family='binomial')
model
summary(model)
shuttle$use
shuttle$use <- as.factor(shuttle$use)
shuttle$use
model <- glm(use~sign,data=shuttle,family='binomial')
shuttle$use
summary(model)
shuttle$sign
shuttle$wind
shuttle$wind <-as.factor(shuttle$wind)
model <- glm(use~wind,data=shuttle,family='binomial')
summary(model)
exp(-0.03181)
log(-0.03181)
model <- glm(use~wind+magn,data=shuttle,family='binomial')
summary(model)
shuttle$magn
summary(model)
exp(-3.201e-2)
test <- shuttle
test$use <- 1-test$use
shuttle$auto
shuttle$auto <- as.numeric(shuttle$use)
shuttle$auto
shuttle$auto <- as.numeric(shuttle$use)-1
shuttle$auto
shuttle$use
shuttle$auto <- -1*(as.numeric(shuttle$use)-2)
shuttle$auto
model <- glm(use~wind,data=shuttle,family='binomial')
summary(model)
exp(-0.03811)
model <- glm(auto~wind,data=shuttle,family='binomial')
summary(model)
exp(0.03181)
shuttle$uwind <- as.numeric(shuttle$wind)
shuttle$uwind
shuttle$wind
exp(-0.03811)
model <- glm(auto~wind,data=shuttle,family='binomial')
summary(model)
exp(0.03181)
model <- glm(auto~uwind,data=shuttle,family='binomial')
summary(model)
model <- glm(auto~wind+magn,data=shuttle,family='binomial')
summary(model)
1/0.96
model <- glm(auto~wind,data=shuttle,family='binomial')
summary(model)
1/0.03181
exp(0.25131)
model <- glm(use~wind,data=shuttle,family='binomial')
summary(model)
exp(-0.25131)
1/exp(-0.25131)
1/1.327
1/0.031
data("InsectSprays")
names(InsectSprays)
head(InsectSprays)
InsectSprays$spray <-as.factor(InsectSprays$spray)
model <- glm(count~spray,data=InsectSprays,family='poisson')
summary(model)
1/0.05588
model <- glm(count~I(1*(spray=='B'))+I(1*(spray=='A')),data=InsectSprays,family='poisson')
summary(model)
0.75/0.81
0.75845/0.81433
exp(0.75845)
exp(0.931)
exp(0.81433)
exp(0.81433/0.75845)
e0.81433/0.75845
0.81433/0.75845
model <- glm(count~spray,data=InsectSprays,family='poisson')
summary(model)
exp(0.05588)
exp(1/0.05588)
1/exp(0.05588)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
plot(x,y)
plot(x(x>0)
)
t = x>0
t
plot(x(t),y(t))
plot(x[t],y[t]])
x
plot(x[5:10],y[5:10])
plot(x[6:10],y[6:10])
x2 <- x[6:10]
y2 <- y[6:10]
lm(y2~x2)
y2
x2
lm(y2~x2)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(caret)
set.seed(33833)
mod1 <- train(y~.,data=vowel.train,method='rf')
mod2 <- train(y~.,data=vowel.train,method='glm')
mod1$finalModel
mod2 <- train(y~.,data=vowel.train,method='glm')
mod2 <- train(y~.,data=vowel.train,method='gbm')
mod2 <- train(y~.,data=vowel.train,method='gbm')
pred1 <- predict(mod1$finalModel,vowel.test$y)
pred1 <- predict(mod1$finalModel,vowel.test)
pred1
confusionMatrix(vowel.test$y,pred1)
set.seed(33833)
mod1 <- train(y~.,data=vowel.train,method='rf')
mod2 <- train(y~.,data=vowel.train,method='gbm')
pred2 <- predict(mod2$finalModel,vowel.test$y)
pred2 <- predict(mod2$finalModel,vowel.test)
mod2$finalModel
pred2 <- predict(mod2$finalModel,vowel.test)
pred2 <- predict(mod2$finalModel,vowel.train)
mod2 <- train(y~.,data=vowel.train,method='gbm')
pred2 <- predict(mod2,vowel.test)
pred2
confusionMatrix(vowel.test$y,pred2)
predDF <- data.frame(pred1,pred2,y=vowel.test$y)
combModFit <- train(y~.,method='gam',data=predDF)
combPred <- predict(combModFit,predDF)
confusionMatrix(predDF$y,combPred)
set.seed(33833)
mod1 <- train(y~.,data=vowel.train,method='rf')
mod2 <- train(y~.,data=vowel.train,method='gbm')
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
training$diagnosis
mod1 <- train(diagnosis~.,data=training,method='rf')
mod2 <- train(diagnosis~.,data=training,method='gbm')
mod3 <- train(diagnosis~.,data=training,method='lda')
confusionMatrix(testing,predict(mod1,testing))
predict(mod1,testing)
confusionMatrix(testing$diagnosis,predict(mod1,testing))
pred1<-predict(mod1,testing)
pred2<-predict(mod2,testing)
pred3<-predict(mod3,testing)
predDF <- data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
combModFit <- train(diagnosis~.,method='rf',data=predDF)
combPred <-predict(combModFit,predDF)
confusionMatrix(testing$diagnosis,combPred)
confusionMatrix(testing$diagnosis,pred1)
confusionMatrix(testing$diagnosis,pred2)
confusionMatrix(testing$diagnosis,pred3)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
names(training)
mod1 <- train(CompressiveStrength~.,data=training,method='lasso')
?plot.enet
names(training)
plot.enet(mod1,xvar=c('Water','Cement','CoarseAggregate','Age'))
plot.enet(mod1$finalModel,xvar=c('Water','Cement','CoarseAggregate','Age'))
plot.enet(mod1$finalModel,xvar=c('step))
plot.enet(mod1$finalModel,xvar=c('step
plot.enet(mod1$finalModel,xvar=c('step'))
plot.enet(mod1$finalModel,xvar=c('fraction'))
plot.enet(mod1$finalModel,xvar=c('penalty'))
plot.enet(mod1$finalModel,xvar=c('penalty'),use.color = TRUE)
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages('lubridate')
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
url <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv'
download.file(url,destfile='~/Desktop/gaData.csv')
download.file(url,destfile='~/Desktop/gaData.csv',method='curl')
https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
install.packages('forecast')
library(forecast)
bats1 <- bats(tstrain)
fcast <- forecast(bats1)
plot(fcast)
accuracy(fcast,testing$visitsTumblr)
?accuracy
help(fcast)
names(fcast)
fcast.upper
fcast$upper
?forecast
fcast$level
tstrain
plot(tstrain)
plot(fcast)
lines(tstrain,col='red')
testing
length(testing)
length(testing$visitsTumblr)
lines(testing$visitsTumblr,col='green')
plot(fcast)
lines(testing,col='red')
lines(testing$va    )
lines(testing$visitsTumblr)
plot(testing)
plot(cast)
plot(fcast)
lines(testing,col='red')
plot(fcast,xlim=c(0,600))
lines(testing,col='red')
lines(testing,col='red')
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
install.packages('e1071')
install.packages("e1071")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(caret)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
set.seed(325)
?"e1071-deprecated"
svm1 <-svm(CompressiveStrength)
names(training)
svm1 <-svm(CompressiveStrength~.,data=training)
pred1 <- predict(svm1,testing)
confusionMatrix(testing$CompressiveStrength,pred1)
pred1
confusionMatrix(testing$CompressiveStrength,pred1)
plot(pred1)
confusionMatrix(testing$CompressiveStrength,pred1)
plot(pred1)
plot(testing$CompressiveStrength,col='red')
plot(pred1)
points(testing$CompressiveStrength,col='red')
sqrt(mean((testing$CompressiveStrength-pred1)^2))
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
bats1 <- bats(tstrain)
library(forecast)
bats1 <- bats(tstrain)
fcast <- forecast(bats1)
fcast
testing
sum(testing$visitsTumblr>774)
sum(testing$visitsTumblr>714)
?forecast
fcast <- forecast(bats1,testing$X)
fcast <- forecast(bats1,h=300)
plot(fcast)
lines(testing,col='red')
fcast
bats1 <- bats(tstrain.method='MMM')
bats1 <- bats(tstrain)
fcast <- forecast(bats1,h=1000)
plot(fcast)
lines(testing,col='red')
testing
forcast
fcast
fcast <- forecast(bats1)
fcast
?forecast
testing
sum(testing$visitsTumblr>774)
sum(testing$visitsTumblr>774)/length(testing$visitsTumblr)
sum(testing$visitsTumblr>714)/length(testing$visitsTumblr)
sum(testing$visitsTumblr>774)/length(testing$visitsTumblr)
?bats
training
testing
600-366
fcast <- forecast(bats1,h-234)
fcast <- forecast(bats1,h=234)
plot(fcast)
lines(testing$X,testing$visitsTumblr,col='red')
fcast
names(fcast)
fcast
res = fcast$mean
res
fcast$upper
fcast$upper[.2]
fcast$upper[.,2]
fcast$upper[,2]
fcast$upper[,2]-testing$visitsTumblr
length(fcast$upper[,2])
length(testing$visitsTumblr)
fcast <- forecast(bats1,h=235)
fcast$upper[,2]-testing$visitsTumblr
er <- fcast$upper[,2]-testing$visitsTumblr
sum(er<0)
length(er)
9/235
9/235*100
100-9/235*100
install.packages('yhat')
install.packages('yhatr')
library(jsonlite)
jsonData<-fromJSON('http://www.theweathernetwork.com/weather/canada/ontario/ottawa')
names(jsonD)
names(jsonData)
jsonData<-fromJSON('http://www.theweathernetwork.com/weather/canada/ontario/ottawa')
library(XML)
fileUrl <- 'http://www.theweathernetwork.com/weather/canada/ontario/ottawa'
doc <- xmlTreeParse(fileUrl)
doc
install.packages(rvest)
install.packages('rvest')
library(rvest)
lego_movie<- html('http://www.imdb.com/title/tt1490017/')
lego_movie<- read_html('http://www.imdb.com/title/tt1490017/')
lego_move
lego_movie
class(lego_movie)
help read_html
read_html?
)_
?read_html
?vignette
vignette('selectorgadget')
weather_url <-'http://www.theweathernetwork.com/weather/canada/ontario/ottawa'
test <- read_html(weather_url)
test <- read_html(weather_url)
names(test)
test.node
test$node
test$doc
test
xml(test)
test <- read_xml(weather_url)
test <- read_html(weather_url)
test <- xmlTreeParse(weather_url)
test <- read_html(weather_url)
library(jsonlite)
test <- fromJSON(weather_url)
test <- read_html(weather_url)
test$node
test$node[1]
test$node[0]
test$node
class(test)
rootNode <- xmlRoot(test$node)
parsed.xml <- xmlTreeParse(test)
parsed.xml
xmlValue(test$node)
xmlValue(test$node[0])
xmlValue(parsed.xml)
xmlValue(parsed.xml[doc])
class(parsed.xml)
parsed.xml$doc
parsed.xml$dtd
parsed.xml$doc[0]
parsed.xml$doc[1]
parsed.xml$doc[2]
parsed.xml$doc[3]
parsed.xml$doc[4]
names(parsed.xml)
xmlValue(parsed.xml$doc[0])
xmlValue(parsed.xml[[1]])
xmlName(parsed.xml)
rootNode <- xmlRoot(parsed.xml)
rootNode
xmlName(rootNode)
names(rootNode)
rootNode[[1]]
rootNode[[0]]
rootNode[[1]]
rootNode[[2]]
rootNode[[3]]
rootNode[[2][1]]
rootNode[[2]]
rootNode[[2]][[2]]
rootNode[[2]][[3]]
rootNode[[2]][[4]]
xmlSApply(rootNode,xmlValue)
rootNode[@temperature
rootNode[@temperature]
rootNode[[3]]
library(jsonlite)
urlvale <- 'api.openweathermap.org/data/2.5/weather?q=Toronto'
tt <- fromJSON(urlvale)
require(rjson)
tt <- fromJSON(urlvale)
urlvale <- 'http://api.openweathermap.org/data/2.5/weather?q=Toronto'
require(rjson)
tt <- fromJSON(urlvale)
urlvale <- "http://api.openweathermap.org/data/2.5/weather?q=Toronto"
tt <- fromJSON(urlvale)
tt <- fromJSON(file=urlvale)
?getURLContent
library(RCurl)
?getURLContent
tt <- fromJSON(getURLContent(urlvale))
require(rjson)
tt <- fromJSON(file=
"http://api.openweathermap.org/data/2.5/history/station?id=7447&type=day")
455513df165308330d1d3707ad1d531f
APIKey <-'455513df165308330d1d3707ad1d531f'
totUrl <- urlvale+'&APPID='+APIKey
totUrl <- urlvale,'&APPID=',APIKey
totUrl <-paste(urlvale,'&APPID=',APIKey)
totUrl
totUrl <-paste(urlvale,'&APPID=',APIKey)
?paste
totUrl <-paste(urlvale,'&APPID=',APIKey,sep='')
totUrl
tt <- fromJSON(getURLContent(totUrl))
tt
urlvale
source('~/Documents/James/Development/R/WeatherApp/UseOpenWeather.R', echo=TRUE)
city.weather
names(city.weather)
city.weather$coord
city.weather$base
city.weather$mian
city.weather$main
head(city.weather)
source('~/Documents/James/Development/R/WeatherApp/UseOpenWeather.R', echo=TRUE)
city.weather
source('~/Documents/James/Development/R/WeatherApp/UseOpenWeather.R', echo=TRUE)
city.weather(Ottawa)
city.weather('Ottawa)
)
''
)
c
fsdfwea
w
erwe
()()
city
fitsr.
re
;
source('~/Documents/James/Development/R/WeatherApp/UseOpenWeather.R', echo=TRUE)
city.weather('ottawa')
data <- city.weather('ottawa')
names(data)
data$coord
data$weather
data$dt
data$base
data
library(stringi)
exit
q()
?sample
?TermDocumentMatrix
library(tm)
?TermDocumentMatrix
The text is reduced to lower-case then punctuation, white space, and numbers are removed and the document is stemmed (suffixes are removed). Finally, profanity is removed. The profanity list used to clean the corpus was complied and posted to a publicly available [blog](http://urbanoalvarez.es/blog/2008/04/04/bad-words-list/
setwd('~/Documents/James/Development/DataScience/Capstone/TextApp/')
ls
library(shiny)
library(rsconnect)
deployApp()
