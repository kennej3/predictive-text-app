---
title: Text Mining Pre-processing 
author: "James Kennedy"
output:
  html_document:
    toc: yes
  fig_caption: yes
  pdf_document:
    fig_height: 4
    fig_width: 5.75
    toc: yes
---
##Synopsis

This document presents the the pre-processing for the predictive text model written in R. The data used in the analysis is presented along with the data filtering and tokenization steps and some exploratory analyses.  

This work uses a number of natural language processing (NLP) libraries in R and acknowledges the tm library documentation [[1](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf),[2](http://www.jstatsoft.org/article/view/v025i05)] and the [CRAN Task View: Natural Language Processing page](http://cran.r-project.org/web/views/NaturalLanguageProcessing.html) as the main sources of reference.

----

##Data Processing

The data processing consists of loading in the data sets, summarizing the data, and creating and filtering the 'corpus' -- an R object used by the text mining (tm) library which handles the collection of text documents and meta-data.

The first step in the analysis is to load the R libraries.
```{r libraries, message=FALSE, warning=FALSE}
library(tm)
library(NLP)
library(RWeka)
library(stringi)
library(SnowballC)
library(slam)
library(ggplot2)
setwd('~/Data/UseData/')
```

###Loading the Data
The data is from HC Corpora [www.corpora.heliohost.org](http://www.corpora.heliohost.org), which creates their corpora from publicly available sources via a web crawler. This analysis uses the US english corpus, saved to the ~/Data directory.

```{r load_data}
setwd('~/Data/final/en_US/')
twitter <- readLines('en_US.twitter.txt',skipNul = TRUE)
news <- readLines('en_US.news.txt', skipNul=TRUE)
blogs <- readLines('en_US.blogs.txt', skipNul=TRUE)
```

To reduce the memory requirements for this exploratory analysis, the data is reduced by randomly sampling 25% of each type before the combined corpus is created.

```{r sample}
set.seed(54321)
twitter <- sample(twitter,size=length(twitter)*0.25)
news <- sample(news,size=length(news)*0.25)
blogs <- sample(blogs,size=length(blogs)*0.25)
total.data <- c(twitter,news,blogs)
rm(twitter)
rm(news)
rm(blogs)
```

### Creating the Corpus

The text is reduced to lower-case then punctuation, white space, and numbers are removed and the document is stemmed (suffixes are removed). Finally, profanity is removed. The profanity list used to clean the corpus was complied and posted to a publicly available [blog](http://urbanoalvarez.es/blog/2008/04/04/bad-words-list/
) by Alejandro U. Alvarez.

```{r Create_Corpus}
Corpus <- VCorpus(VectorSource(total.data))
Corpus <- tm_map(Corpus, content_transformer(tolower),lazy=TRUE)
Corpus <- tm_map(Corpus, removePunctuation,lazy=TRUE)
Corpus <- tm_map(Corpus, stripWhitespace,lazy=T)
Corpus <- tm_map(Corpus, removeNumbers)
#Corpus <- tm_map(Corpus, stemDocument)
profanity_list <- readLines('~/Data/profanity_list/badwords.txt')
Corpus <- tm_map(Corpus, removeWords, profanity_list, lazy=TRUE)
```

--- 

## Tokenization and creating N-grams
The data is now tokenized, meaning that it is divided into groups of consecutive words. These groups are called [N-grams](https://en.wikipedia.org/wiki/N-gram), where the 'N' refers to the number of words in the group.  Here, the NGramTokenizer function is used to create N-grams with N = 1,2, and 3 (unigrams, bigrams, and trigrams). These are simultaneously coerced into a term-document matrix for analysis.

```{r Tokenize}
options(mc.cores=1)
GramTokenizeOne <- function(x){NGramTokenizer(x, Weka_control(min=1,max=1))}
GramTokenizeTwo <- function(x){NGramTokenizer(x, Weka_control(min=2,max=2))}
GramTokenizeThree <- function(x){NGramTokenizer(x, Weka_control(min=3,max=3))}
GramTokenizeFour <- function(x){NGramTokenizer(x, Weka_control(min=4,max=4))}

gram.N1 <- TermDocumentMatrix(Corpus,control=list(tokenize=GramTokenizeOne))
save(gram.N1, file='~/Data/UseData/N1Data.RData')
rm(gram.N1)

gram.N2 <- TermDocumentMatrix(Corpus,control=list(tokenize=GramTokenizeTwo))
save(gram.N2, file='~/Data/UseData/N2Data.RData')
rm(gram.N2)

gram.N3 <- TermDocumentMatrix(Corpus,control=list(tokenize=GramTokenizeThree))
save(gram.N3, file='~/Data/UseData/N3Data.RData')
rm(gram.N3)

gram.N4 <- TermDocumentMatrix(Corpus,control=list(tokenize=GramTokenizeFour))
save(gram.N4, file='~/Data/UseData/N4Data.RData')
rm(gram.N4)

rm(Corpus)
```

